{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrozenLake environment consists of a 4 by 4 grid representing a surface. The agent always starts from the state 0, [0,0] in the grid, and his goal is to reach the state 16, [4,4] in the grid. On his way, he could find some frozen surfaces or fall in a hole. If he falls, the episode is ended. When the agent reaches the goal, the reward is equal to one. Otherwise, it is equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "n_observations = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize the Q-table to 0\n",
    "Q_table = np.zeros((n_observations,n_actions))\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma (γ):\n",
    "Gamma is the discount factor, and it represents the extent to which future rewards should be considered in the decision-making process.\n",
    "It is a value between 0 and 1, where 0 means the agent only considers immediate rewards, and 1 means the agent considers all future rewards equally.\n",
    "By discounting future rewards, the algorithm accounts for the fact that future rewards are generally worth less than immediate rewards. This is because the environment is uncertain, and there is a possibility that the agent might not reach future states.\n",
    "\n",
    "Learning Rate (α):\n",
    "The learning rate is a parameter that controls the step size in updating the Q-values based on new information.\n",
    "It is a value between 0 and 1 and determines the proportion of the difference between the old and new Q-values that is used to update the old Q-value.\n",
    "A higher learning rate allows the agent to give more weight to the most recent information, making the learning process more volatile. On the other hand, a lower learning rate results in slower but more stable learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of episode we will run\n",
    "n_episodes = 10000\n",
    "\n",
    "#maximum of iteration per episode\n",
    "max_iter_episode = 100\n",
    "\n",
    "#initialize the exploration probability to 1\n",
    "exploration_proba = 1\n",
    "\n",
    "#exploartion decreasing decay for exponential decreasing\n",
    "exploration_decreasing_decay = 0.001\n",
    "\n",
    "# minimum of exploration proba\n",
    "min_exploration_proba = 0.01\n",
    "\n",
    "#discounted factor\n",
    "gamma = 0.99\n",
    "\n",
    "#learning rate\n",
    "lr = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards_episode = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we iterate over episodes\n",
    "rewards_per_episode = []\n",
    "for e in range(n_episodes):\n",
    "    #we initialize the first state of the episode\n",
    "    current_state,_ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    #sum the rewards that the agent gets from the environment\n",
    "    total_episode_reward = 0\n",
    "    \n",
    "    for i in range(max_iter_episode): \n",
    "        # we sample a float from a uniform distribution over 0 and 1\n",
    "        # if the sampled flaot is less than the exploration proba\n",
    "        #     the agent selects arandom action\n",
    "        # else\n",
    "        #     he exploits his knowledge using the bellman equation \n",
    "        \n",
    "        if np.random.uniform(0,1) < exploration_proba:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state,:])\n",
    "        \n",
    "        # The environment runs the chosen action and returns\n",
    "        # the next state, a reward and true if the epiosed is ended.\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "    \n",
    "        # We update our Q-table using the Q-learning iteration\n",
    "        Q_table[current_state, action] = (1-lr) * Q_table[current_state, action] +lr*(reward + gamma*max(Q_table[next_state,:]))\n",
    "        total_episode_reward = total_episode_reward + reward\n",
    "        # If the episode is finished, we leave the for loop\n",
    "        if done:\n",
    "            break\n",
    "        current_state = next_state\n",
    "    #We update the exploration proba using exponential decay formula \n",
    "    exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay*e))\n",
    "    rewards_per_episode.append(total_episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward per thousand episodes\n",
      "1000 : mean espiode reward:  0.033\n",
      "2000 : mean espiode reward:  0.215\n",
      "3000 : mean espiode reward:  0.432\n",
      "4000 : mean espiode reward:  0.599\n",
      "5000 : mean espiode reward:  0.655\n",
      "6000 : mean espiode reward:  0.692\n",
      "7000 : mean espiode reward:  0.676\n",
      "8000 : mean espiode reward:  0.664\n",
      "9000 : mean espiode reward:  0.69\n",
      "10000 : mean espiode reward:  0.668\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean reward per thousand episodes\")\n",
    "for i in range(10):\n",
    "    print((i+1)*1000,\": mean espiode reward: \",\\\n",
    "           np.mean(rewards_per_episode[1000*i:1000*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
